{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB LSTM EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB dataset [explaination](https://keras.io/datasets/)\n",
    "\n",
    "  Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "x_train shape: (25000,)\n",
      "x_test shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer\n",
    "\n",
    " From keras document:\n",
    "  Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "    eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "\n",
    "    This layer can only be used as the first layer in a model.\n",
    "\n",
    "    ```python\n",
    "      model = Sequential()\n",
    "      model.add(Embedding(1000, 64, input_length=10))\n",
    "      # the model will take as input an integer matrix of size (batch, input_length).\n",
    "      # the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
    "      # now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "\n",
    "      input_array = np.random.randint(1000, size=(32, 10))\n",
    "\n",
    "      model.compile('rmsprop', 'mse')\n",
    "      output_array = model.predict(input_array)\n",
    "      assert output_array.shape == (32, 10, 64)\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "WARNING:tensorflow:From /Users/chenjunzou/project/boltzmann-machines/venv/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/chenjunzou/project/boltzmann-machines/venv/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 147s 6ms/step - loss: 0.4712 - acc: 0.7746 - val_loss: 0.4014 - val_acc: 0.8214\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 149s 6ms/step - loss: 0.3022 - acc: 0.8770 - val_loss: 0.4197 - val_acc: 0.8093\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 138s 6ms/step - loss: 0.2152 - acc: 0.9182 - val_loss: 0.4329 - val_acc: 0.8331\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 130s 5ms/step - loss: 0.1510 - acc: 0.9439 - val_loss: 0.4768 - val_acc: 0.8244\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 130s 5ms/step - loss: 0.1068 - acc: 0.9616 - val_loss: 0.5162 - val_acc: 0.8243\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 130s 5ms/step - loss: 0.0800 - acc: 0.9713 - val_loss: 0.6280 - val_acc: 0.8236\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 130s 5ms/step - loss: 0.0610 - acc: 0.9800 - val_loss: 0.7167 - val_acc: 0.8179\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 125s 5ms/step - loss: 0.0430 - acc: 0.9860 - val_loss: 0.7400 - val_acc: 0.8207\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 121s 5ms/step - loss: 0.0341 - acc: 0.9890 - val_loss: 0.8002 - val_acc: 0.7970\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 120s 5ms/step - loss: 0.0231 - acc: 0.9932 - val_loss: 0.9322 - val_acc: 0.8175\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 121s 5ms/step - loss: 0.0223 - acc: 0.9930 - val_loss: 0.8860 - val_acc: 0.8064\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 121s 5ms/step - loss: 0.0169 - acc: 0.9946 - val_loss: 1.1153 - val_acc: 0.8116\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 121s 5ms/step - loss: 0.0133 - acc: 0.9956 - val_loss: 1.0606 - val_acc: 0.8109\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 121s 5ms/step - loss: 0.0089 - acc: 0.9972 - val_loss: 1.1239 - val_acc: 0.8106\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 120s 5ms/step - loss: 0.0112 - acc: 0.9964 - val_loss: 1.1035 - val_acc: 0.7790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11ca64290>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 16s 645us/step\n",
      "Test score: 1.1034785650253296\n",
      "Test accuracy: 0.779\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
